---
title: DDIA Chapter7 Transctions 谈谈当年不太理解的知识
date: 2024-02-14 21:46:45
tags: DDIA
---

年少时阅读mysql文档，被事务机制困扰了不少。总觉得是一种死记硬背的知识，现在发现并不是。

这篇blog的脉络是：探讨基本的ACID概念，然后探讨Isolation面临的问题和解法。

# Transactions

## ACID 基本概念

事务由一系列读写操作组合在一起，主要关注数据库能给这些读写操作提供什么保证。

* Atomic 原子性
    * 保证读写操作必须全部执行成功，或者全部失败（被回滚 Rollback/Abort，对db没有任何影响）
* Consistence 一致性
    * 保证数据状态完好无损。
    * 一方面，db可以保证数据取值遵循约定好的规则（长度/大小等），如果违反规则，拒绝存储
    * 另一方面，db的使用者（Application）必须保证，数据写入之后，整体状态没有损坏。比如Application约定只能有10个获奖者，结果自己写入了11个，这是违反一致性的。
    * **这个特性更多偏向Application，跟DB没什么关系。db只能保证基本的数据类型不被违反，但是不理解数据的含义；需要Application自己定义什么是“状态完好无损”。**
* Isolation 隔离
    * 关注不同事务之间的读写操作如何相互影响，这也是我们最感兴趣的和最复杂的点。
* Durability 持久性
    * 保证数据被db完毕后，可以一直保存不出问题。
    * 其实这是不可能的，db只能在有限的条件下保证：如果磁盘/电源等不出问题，那么单机数据永不丢失；如果集群节点大部分可用，那么数据在集群范围内可用。

## Weak Isolation Levels 隔离级别-前人的一些努力
 
其实主要就是探讨Isolation。db为了保证隔离性，需要做出很多努力，才能达到一定程度的隔离性；不同事务的读写操作会相互影响，在不同的隔离级别下带来不同的问题。

* level0 什么也不做
    * 完全没有隔离可言。一个事务的写操作可以被另一个事务里的读操作观察到，Application将拥有一个不稳定的db，随时爆炸。
* level1 Read Committed 读已提交
    * 针对level0，事务A读取到了事务B的数据（Dirty Read，脏读），我们自然想到要对事务A屏蔽事务B的写操作。这很好实现，经典做法就是保存数据的2个版本：事务A把数据从version 1更新到version 2后，只要事务A未提交，事务B就只能读取到version 1，而不是version2；等到事务A提交完成，db在把version1干掉，只保留version。这个机制叫做**Snapshot version of old data**。
    * 针对两个事务的写操作，还要保证它们不要相互影响。典型场景是，事务A/B分别要把2个字段更新为Value1/Value2 Value3/Value4，如果它们互不影响，最终的数据应该是Value1/Value2 or Value3/Value4，两种可能；但是犹豫更新顺序、时机不同，最终结果可能是A/B混合在一起，比如Value1/Value4 or Value3/Value2，此时数据已经坏掉了。这种称为脏写(Dirty Write)。一个简单的解决办法是加锁，比如Mysql/Innodb有row locks，写数据时获得锁，事务完成后释放锁，如此就不会发生Dirty Write。
* level2 Repeatable Read 可重复读
    * level1是有缺陷的，比如事务A第一次读取到了数据x，但是事务B把x更新为y并完成提交，此时事务A再去读取，会获得数据y。此问题成为不可重复度（Unrepeatable Read），数据变成非常不稳定的状态。类似level1引入的snapshot，我们再引入多个版本，只要不主动修改，每个事务里读取的数据总是不变。即多版本控制，（MVCC Multi Version Concurrency Control）
    * MVCC通常这样实现：事务带有自增版本号txid，每条数据加上createdby/deletedby两个字段，承载修改/删除当前数据的txid。**查询数据时，通过比较当前事务和数据的createdby/deletedby的大小，决定是否可见。**比如txid13写入了数据a，txid15把a更新为b，那么txid14可以读取到a（因为txid13小于txid14），但是txid14不能获取到数据b（因为txid14小于txid15）。事务提交后，db后台进程会自动回收不需要保存的数据版本。

看起来level2已经很完美了，但是非常遗憾，还有一些未解决的问题。比如：
  * LostUpdates 两个事务并发的写操作，到底谁生效？
    * 譬如两个事务都想把一个counter字段+1，但是因为大家的逻辑都是```select counter;got value x;update counter=x+1```，即使已经有了row locks，两个事务执行完毕后，counter还是会变为counter+1，而不是counter+2。这种问题可以通过Atomic Write Operations解决，简单来说就```update counter=counter+1```，两个事务个执行一遍，最后结果变为counter+2。
    * 对于非counter的场景，比如事务A想把角色a移动到x位置，事务B也想把角色b移动到x位置，他们的逻辑是```select x;check if x is empty;then move a/b to x```。最终结果是A/B都以为成功移动了角色，实际上只有其中一个成功移入到x。这种逻辑可以概括为，查询数据-判断-写入，第三步写入会影响第一步的查询结果。也就是说当我们要查询某个数据然后决定是否修改时，要防止其他事务也做类似的操作。这时就需要引入**显示锁（Explicit Locks）**，也就是```select x for update```，在事务A执行第一步查询数据时，直接加锁，这样事务B就需要等待了。
  * Write Skew and Phantoms，幻读问题
    * 再次遗憾，```select for udpate```并不能解决所有LostUpdates问题。比如没有匹配到的selelct结果，也就无法加显式锁了。试想这样的场景，事务A查询是否有20-25岁的参赛者，如果没有就把青年组球赛取消；事务B向db增加一位23岁参赛选手。事务A并不能锁出20-25这个条件下的对象，因为这样的对象还不存在；等到事务A查询完成，事务B提交完成，A再去取消青年组就不合适了。如果事务A重续查询20-25这个条件，会发现莫名其妙出现一位新选手，这就是幻读问题：**db在此刻变得前后不一致了，隔离性再次被打破。**


不过我们依然有解决方案，就是锁住20-25这个并不存在的数据。
* 预测锁 （Predicate Locks）。我们直接把查询条件作为一种锁，当事务A使用了20-25这个条件，我们为它建立一个锁X；事务B插入数据时，我们检查这个数据是否命中X的条件，如果命中，就等待事务A完成，再继续查询。如此就锁住了“不存在的数据“。
如果db处理的事务相对简单，这种方法很有效。不过随着查询条件复杂化，以及长事务的增多，预测锁会带来性能问题：每个select都要去检查是否命中条件，这是一件非常消耗CPU时间的事情。
* 间隙锁（Index Lock, or Next-Key lock）。鉴于大部分ACID db都基于B树索引，select查询又和B树索引的某些区间匹配，那么把这些区间的启始key锁住，其他任何发生在这个区间里的写入操作都需要等待间隙锁释放才能完成。如此，我们在写入数据时，不必再参考各种奇怪的预测锁（它们的命中条件往往需要一些计算工作），而是直接看当前索引区间是否被锁住。这也是Mysql/Innodb在做的事情。

现在我们有了行级别的读写锁/快照机制/间隙锁，看起来已经足够解决各种隔离问题。但是这些工具带来另一个问题：我们作为Appliercation的开发者，真的能自如使用这些复杂工具么？比如发生死锁怎么去排查？如果我的代码里充斥着```select xxx for update``` / ```这里使用了间隙锁特性``` 类似的sql和注释，那简直太令人头疼了，都不好去debug。

如果不想应对这些锁带来的复杂度，有两个办法：**提高设计，我们不要引入这些竞争和隔离问题，尽量减少可能冲突的场景，我认为这是最合理的路线**；另一个办法是db提供帮助，直接进入到level3，串行化！

## Serializability 可串行化-新的思路待检验

db领域有大量的天才，他们提出**可串行化**这个词：虽然多个事务同时进行读写，但是db保证最终效果和串行执行相同。实现方法也有三个。

### 最质朴的想法：真的贯彻串行化思想（Actual Serial Excution）

不允许并发执行事务。这种想法在某些领域是可行的，特别是当内存变得cheap，No-SQL领域可以把数据完全存储在内存，串行可能带来更高的效率。比如Redis就是这样。可以还有很多关系型DB，面对的是TB级别数据，完全无法忍受串行带来的效率降低。

### 花哨的想法：二阶段锁（Two Phase Locks，2PL）
事务A在读取数据时，把查询结果加上读锁；如果事务A后来需要更新他们，就把读锁升级为写锁；当事务A完成时，释放所有读写锁。如果有事务B在同步执行，他必须等待事务A获得的写锁完全释放，才能执行查询操作；而事务B的写操作，也要等待事务A的所有读/写锁释放才能进行。在Mysql/Innodb的默认配置，也就是可重复读的级别下，引入了2PL。

但是2PL会带来性能问题：加锁/释放本来就有一定开销；特别是db负载较高时，死锁概率升高，很可能事务A经过了大量操作后，突然发现死锁，需要回滚再重试，这带来了额外的开销（**所以重试其实并不是什么灵丹妙药！**）。

2PL一般搭配着间隙锁的概念实现，至少Mysql官方文档是这样描述他们的InnoDB。描述归描述，我们其实没有必要掌握着么细节的东西，还是得从根本上解决问题（也就是设计角度）。

### 换个乐观注意的想法：串行快照隔离（Serialzable Snapshot Isolation， SSI）

前文描述的各种锁，其实是悲观主义的产物：我们总是认为会发生冲突和隔离问题，所以强制加锁。对应的，我们还有一种乐观的想法：我们认为不会发生冲突和隔离问题，所以在数据更新时只是简单记录下这个事件，等到有查询用到被更新的数据，再去判断是否有隔离问题。

到这一步，我本人已经不想继续学习了。过于复杂的机制，完全让人无法把握！DDIA也说明，这是一种新兴的可串行化实现算法，还在被实践检验的阶段（虽然DDIA是偏乐观的态度）。如果将来的开发者时时刻刻要考虑这个隔离级别的实现细节，那是不可能承受的复杂度！但是从学习的角度，还是可以探讨下。。

那么如何“判断是否有隔离问题”？有两种场景：
* Detecting Stale MVCC reads。基于MVCC读取数据时，我们记录下有哪些过于新的txid，等到事务提交时，再去见检查这些txid是否已经提交完成。如果已经提交，那么当前事务使用了陈旧的MVCC版本（stale），需要回滚当前事务。
* Detecting Writes that affect prior reads。事务在写数据时，主动通知读取了对应数据的事务。注意这里只是简单的通知而已，并不会阻塞或者导致回滚。这个机制完全是为了补充上一条。

好了，SSI已经十分复杂。对比2PL，SSI总是在事务提交时去检查冲突，如果冲突较少，会使得大部分的事务保持相对短的处理事件，这挺棒的；对比Actual Serial Execution，它提高了效率，只要cpu数量得到扩展，SSI就能跟着提高性能。SSI真棒！（谁愿意谁实现吧，老旧的Mysql估计是不会改变的）。
